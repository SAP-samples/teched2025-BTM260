{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c5b916",
   "metadata": {},
   "source": [
    "# Exercise 1: Basic LeanIX AI Agent\n",
    "\n",
    "## Goal\n",
    "\n",
    "We want you to make yourself familiar with LeanIX and to learn how you can find information about Enterprise Applications of your company both in the LeanIX workspace as well as via an AI Agent based on MCP.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "Your task is to answer the following Enterprise Architecture question: \n",
    "\n",
    "**Which applications have a perfect functional fit and fully appropriate technical fit?**\n",
    "\n",
    "- first in LeanIX directly\n",
    "- later with an AI Agent via MCP\n",
    "\n",
    "We will go through these steps:\n",
    "\n",
    "1. Find the information in LeanIX\n",
    "1. Create LeanIX API token\n",
    "1. Use the token to connect to the LeanIX MCP Server\n",
    "1. Filter the MCP tools \n",
    "1. Connect to an LLM on AI Core\n",
    "1. Create an AI Agent\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9591bc",
   "metadata": {},
   "source": [
    "## Task 1.1: Find information in LeanIX\n",
    "\n",
    "In order to answer the above question in LeanIX, we will navigate to the workspace and use a pre-built report to find the applications that match the defined criteria.\n",
    "\n",
    "**ðŸš§ Your task:** Please follow these steps\n",
    "\n",
    "1. Log in to your LeanIX workspace - *todo: URL*\n",
    "1. Navigate to Reports and select Application Portfolio report\n",
    "1. Clear the filters for lifecycle and quality seal\n",
    "1. Hover over the bubble in the upper right corner\n",
    "1. Click on the bubble in the upper right corner\n",
    "1. Click on \"Show in Inventory\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67584ec",
   "metadata": {},
   "source": [
    "### âœ… Solution\n",
    "<details>\n",
    "<summary>Click to expand solution</summary>\n",
    "\n",
    "![Screenshot portfolio report](../../images/portfolio-report.png)\n",
    "\n",
    "10 applications match these criteria\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c85928",
   "metadata": {},
   "source": [
    "## Task 1.2: Create LeanIX API token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b843657",
   "metadata": {},
   "source": [
    "In order for us to access the same information via an external AI Agent, we need to create an API token. \n",
    "\n",
    "**ðŸš§ Your task:** Please follow these steps:\n",
    "\n",
    "1. In LeanIX, click on the user initials in the upper right corner\n",
    "1. Select Administration\n",
    "1. In the section \"Discovery and Integrations\" select \"Technical Users\"\n",
    "1. Click on \"New Technical User\"\n",
    "1. As a name type in \"mcp`nnn`\" with `nnn` being your assigned user number in the session (e.g., mcp001)\n",
    "1. For permission role select \"MEMBER\"\n",
    "1. Set the expiry date to the last day of the current month\n",
    "1. Save the technical user and copy the API key \n",
    "1. Paste the API key to your `.env` file and assign it to the environment variable `LEANIX_API_TOKEN`\n",
    "\n",
    "Your technical user configuration should look similar to this:\n",
    "\n",
    "![Technical user](../../images/technical-user.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03463bff",
   "metadata": {},
   "source": [
    "## Task 1.3: Connect to LeanIX MCP Server\n",
    "\n",
    "Let's connect to the LeanIX MCP Server. We will use the API token from the previous step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfec190",
   "metadata": {},
   "source": [
    "#### Load API token\n",
    "\n",
    "Let's first load the API token from the environment.\n",
    "\n",
    "**ðŸš§ Your task:** Execute the cell below and make sure it's loaded successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5b887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "LEANIX_API_TOKEN = os.getenv(\"LEANIX_API_TOKEN\")\n",
    "\n",
    "if LEANIX_API_TOKEN:\n",
    "    print(\"LEANIX_API_TOKEN loaded successfully.\")\n",
    "else:\n",
    "    print(\"Warning: LEANIX_API_TOKEN is not set or empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad9279b",
   "metadata": {},
   "source": [
    "#### Connect to MCP Server\n",
    "\n",
    "Now connect to the LeanIX MCP Server. The code below creates an MCP Client using the LangChain library to connect and get the tools. \n",
    "\n",
    "**ðŸš§ Your task:** Execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d8ebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "lx_api_token_base64 = base64.b64encode(\n",
    "    f\"apitoken:{LEANIX_API_TOKEN}\".encode()\n",
    ").decode()\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"LeanIX MCP Remote\": {\n",
    "            \"transport\": \"streamable_http\",\n",
    "            \"url\": \"https://demo-eu-2.leanix.net/services/mcp-server/v1/mcp\",\n",
    "            \"headers\": {\"Authorization\": f\"Basic {lx_api_token_base64}\"},\n",
    "        }\n",
    "        # Add more servers as needed\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()\n",
    "\n",
    "tool_data = [{\n",
    "    'Name': tool.name,\n",
    "    'Description': tool.description\n",
    "} for tool in tools]\n",
    "\n",
    "# Create DataFrame from tool data\n",
    "df_tools = pd.DataFrame(tool_data)\n",
    "\n",
    "# Truncate Description column to 200 characters\n",
    "df_tools['Description'] = df_tools['Description'].apply(lambda x: x[:200] + ('...' if len(x) > 200 else ''))\n",
    "\n",
    "# Show all text in columns and align left for readability\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "styles = [\n",
    "    {\"selector\": \"th\", \"props\": [(\"text-align\", \"left\")]},\n",
    "    {\"selector\": \"td\", \"props\": [(\"text-align\", \"left\")]}\n",
    "]\n",
    "\n",
    "# Display the table\n",
    "display(HTML(df_tools.style.set_table_styles(styles).to_html()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e906bea",
   "metadata": {},
   "source": [
    "You should now see the tools available on the MCP Server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad24529",
   "metadata": {},
   "source": [
    "## Task 1.4: Filter the MCP Tools\n",
    "\n",
    "It is good practice to do some client-side filtering of tools as too many tools can overwhelm an LLM. Let's do some very basic filtering just based on keywords for the moment. In a real production system, we would rather implement a small Tools RAG.\n",
    "\n",
    "**Your Task:**\n",
    "\n",
    "Create a list `filtered_tools` which contains only the tools that have the word \"application\" in their description. Print the tools details to make yourself familiar with the selected tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba3bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ac04b0",
   "metadata": {},
   "source": [
    "### âœ… Solution\n",
    "<details>\n",
    "<summary>Click to expand solution</summary>\n",
    "\n",
    "```python\n",
    "filtered_tools = [\n",
    "    t for t in tools\n",
    "    if \"application\" in (t.description or \"\").lower() \n",
    "]\n",
    "\n",
    "print(f\"Filtered tools ({len(filtered_tools)}):\")\n",
    "for tool in filtered_tools:\n",
    "    print(f\"Tool: {tool.name}\")\n",
    "    print(tool)\n",
    "    print(\"------\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1244fd86",
   "metadata": {},
   "source": [
    "## Task 1.5: Connect to an LLM via AI Core\n",
    "\n",
    "Each MCP-based scenario always involves three players: \n",
    "- One or multiple MCP Servers \n",
    "- A Hosting app which connects to the MCP Servers and runs the AI Agents\n",
    "- An LLM which makes the tooling decisions and generates the final answers to user requests\n",
    "\n",
    "We will now add the LLM to the game in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1121016",
   "metadata": {},
   "source": [
    "#### Initialize AI Core\n",
    "\n",
    "We start with loading the AI Core credentials and connecting to an LLM.\n",
    "\n",
    "**ðŸš§ Your task:** Check that all environment variables are set in the .env file, then execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c87e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain import init_llm\n",
    "\n",
    "# TODO: delpoy workshop model(s) and test again\n",
    "\n",
    "AICORE_CLIENT_ID = os.getenv(\"AICORE_CLIENT_ID\")\n",
    "AICORE_CLIENT_SECRET = os.getenv(\"AICORE_CLIENT_SECRET\")\n",
    "AICORE_RESOURCE_GROUP = os.getenv(\"AICORE_RESOURCE_GROUP\")\n",
    "AICORE_BASE_URL = f\"{os.getenv('AICORE_BASE_URL')}/v2/lm\"\n",
    "AICORE_AUTH_URL = f\"{os.getenv('AICORE_AUTH_URL')}/oauth/token\"\n",
    "\n",
    "# Check if all variables are set (non-empty)\n",
    "required_vars = [\n",
    "    ('AICORE_CLIENT_ID', AICORE_CLIENT_ID),\n",
    "    ('AICORE_CLIENT_SECRET', AICORE_CLIENT_SECRET),\n",
    "    ('AICORE_RESOURCE_GROUP', AICORE_RESOURCE_GROUP),\n",
    "    ('AICORE_BASE_URL', AICORE_BASE_URL),\n",
    "    ('AICORE_AUTH_URL', AICORE_BASE_URL),\n",
    "]\n",
    "missing = [name for name, val in required_vars if not val]\n",
    "if missing:\n",
    "    print(f\"Warning: The following AI Core environment variables are not set: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"All required AI Core environment variables are set.\")\n",
    "\n",
    "\n",
    "llm = init_llm(\"gpt-4.1\", max_tokens=32767)\n",
    "\n",
    "# Structured print of important LLM fields\n",
    "fields = [\n",
    "    ('model_name', getattr(llm, 'model_name', None)),\n",
    "    ('max_tokens', getattr(llm, 'max_tokens', None)),\n",
    "    ('temperature', getattr(llm, 'temperature', None)),\n",
    "]\n",
    "print(\"LLM Configuration\")\n",
    "for name, value in fields:\n",
    "    if value is not None:\n",
    "        print(f\"  {name}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82369bbd",
   "metadata": {},
   "source": [
    "#### Test the LLM\n",
    "\n",
    "We now need to test if the LLM works as expected.\n",
    "\n",
    "**ðŸš§ Your task:** Invoke the `llm` instance with a test prompt and extract token usage from the response metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55e8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import pprint\n",
    "\n",
    "# TODO: ðŸš§ Invoke the LLM with a test prompt\n",
    "test_prompt = \"\"\n",
    "response = llm.invoke(test_prompt)\n",
    "\n",
    "# Print the output as Markdown\n",
    "display(Markdown(f\"**LLM Response:**\\n\\n{response.content}\"))\n",
    "\n",
    "# Debug print the full response object to understand its structure\n",
    "pprint.pprint(\"Use this structure to find the right total tokens field\")\n",
    "pprint.pprint(f\"debug string: {response}\")\n",
    "\n",
    "# TODO: Bonus ðŸš§ Extract the number of tokens used from the response metadata\n",
    "# total_tokens = \n",
    "# display(Markdown(f\"`Total tokens used: {total_tokens}`\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614d7383",
   "metadata": {},
   "source": [
    "### âœ… Solution\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand solution</summary>\n",
    "\n",
    "```python\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "response = llm.invoke(\"Hello world\")\n",
    "\n",
    "# Print the main response content as Markdown\n",
    "display(Markdown(f\"**LLM Response:**\\n\\n{response.content}\"))\n",
    "\n",
    "# Print the token usage in a highlighted way\n",
    "total_tokens = response.response_metadata[\"token_usage\"][\"total_tokens\"]\n",
    "display(Markdown(f\"`Total tokens used: {total_tokens}`\"))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8ea35f",
   "metadata": {},
   "source": [
    "## Task 1.6: Create AI Agent\n",
    "\n",
    "Now we want to tackle the 3rd player of the MCP scenario and create an AI Agent that exposes MCP tools to an LLM to answer questions. We use a ReAct Agent from the LangGraph library for this purpose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81128b13",
   "metadata": {},
   "source": [
    "#### Understanding ReAct Agents\n",
    "\n",
    "A **ReAct agent** (Reason + Act) is an AI agent that can both reason about problems and take actions using external tools. In LangGraph, a ReAct agent can:\n",
    "\n",
    "- Think step-by-step about a user's question\n",
    "- Decide when to use a tool (like searching in LeanIX)\n",
    "- Combine information from multiple sources\n",
    "- Respond in a way that's easy for employees to understand\n",
    "\n",
    "The agent acts as a knowledgeable guide that knows how to look up information and explain it clearly to internal users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba71b92",
   "metadata": {},
   "source": [
    "#### Initialize AI Agent\n",
    "\n",
    "We define the a LangGraph ReAct Agent with the cell below. \n",
    "\n",
    "**ðŸš§ Your task:** Run the cell below to create the Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb0540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from IPython.display import Markdown, display, Image \n",
    "\n",
    "agent =  create_react_agent(llm, filtered_tools)\n",
    "print(f\"Initialized Agent: {agent}\")\n",
    "\n",
    "async def call_agent(agent, query):\n",
    "    response = await agent.ainvoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def print_agent_response(response):\n",
    "    msgs = response.get(\"messages\", [])\n",
    "    for msg in reversed(msgs):\n",
    "        content = getattr(msg, \"content\", \"\")\n",
    "        if content:\n",
    "            display(Markdown(content))\n",
    "            break\n",
    "    else:\n",
    "        print(\"No printable content in agent response.\")\n",
    "\n",
    "\n",
    "display(Image(agent.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eab8c1",
   "metadata": {},
   "source": [
    "You can see the structure of the Agent in the graph. It's a simple loop deciding on tools and calling tools until the Agent thinks it has reached its goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195a535",
   "metadata": {},
   "source": [
    "#### Call the Agent\n",
    "\n",
    "**Task:** Now call the Agent to answer the question \"Which applications have a perfect functional fit and fully appropriate technical fit?\" and print the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4075fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6821f01",
   "metadata": {},
   "source": [
    "### âœ… Solution\n",
    "<details>\n",
    "<summary>Click to expand solution</summary>\n",
    "\n",
    "```python\n",
    "response = await call_agent(\n",
    "    agent, \n",
    "    \"Which applications have a perfect functional fit and fully appropriate technical fit?\"\n",
    ")\n",
    "print_agent_response(response)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2812441",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully mastered this exercise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
